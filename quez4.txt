is the most important task in order to reduce the cost of mistakes and to find and fix architectural problems as early as possible. 
 includes the application type, the deployment architecture, the architectural style, technology choices, quality attributes, and crosscutting concerns.
 makes sure that all the developers working on the project are following certain specified guidelines. The code can be easily understood and proper consistency is maintained.
  is similar to the lead time with a difference that leads time is measured per user story, while cycle time is measured per task. For eg, if database creation is part of the user story related to client data.
  is the union of people, process, and products to enable continuous delivery of value to our customers.
   is defined as the time it takes from the time of project or sprint kick-off to the completion. In an agile process, we normally pick up user stories that will be delivered at the end of the sprint.
are a dominant method for reviewing an architecture design which focuses on the scenarios that are most important from the business perspective, and which have the greatest impact on the architecture.
can be defined as the ability of the software to function as per user requirement.  When it comes to software products it must satisfy all the functionalities written down in the software requirement specification document. 
 is a very important metric for Agile/Scrum. It is an indicator of the number of tasks or user stories a team is able to complete during a single sprint.  
 is one of the parts of the maintenance wherein during the process of software development corrective fault not found or discovered.
 is one of the parts of the maintenance wherein under this step the software functions increased according to the need of customer.
  is one of the parts of the maintenance wherein the software is transformed to new operating system, environments or to a new computer.
  This is the first step where the user initiates the request for a desired software product. He contacts the service provider and tries to negotiate the terms. He submits his request to the service providing organization in writing.
   This step onwards the software development team works to carry on the project. The team holds discussions with various stakeholders from problem domain and tries to bring out as much information as possible on their requirements. The requirements are contemplated and segregated into user requirements, system requirements and functional requirements.
 After requirement gathering, the team comes up with a rough plan of software process. At this step the team analyzes if a software can be made to fulfil all requirements of the user and if there is any possibility of software being no more useful. It is found out, if the project is financially, practically and technologically feasible for the organization to take up. There are many algorithms available, which help the developers to conclude the feasibility of a software project.
  At this step the developers decide a roadmap of their plan and try to bring up the best software model suitable for the project. System analysis includes Understanding of software product limitations, learning system related problems or changes to be done in existing systems beforehand, identifying and addressing the impact of project on organization and personnel etc. The project team analyzes the scope of the project and plans the schedule and resources accordingly.
  Next step is to bring down whole knowledge of requirements and analysis on the desk and design the software product. The inputs from users and information gathered in requirement gathering phase are the inputs of this step. The output of this step comes in the form of two designs; logical design and physical design. Engineers produce meta-data and data dictionaries, logical diagrams, data-flow diagrams and in some cases pseudo codes.
  This step is also known as programming phase. The implementation of software design starts in terms of writing program code in the suitable programming language and developing error-free executable programs efficiently.
  An estimate says that 50% of whole software development process should be tested. Errors may ruin the software from critical level to its own removal. Software testing is done while coding by the developers and thorough testing is conducted by testing experts at various levels of code such as module testing, program testing, product testing, in-house testing and testing the product at user’s end. Early discovery of errors and their remedy is the key to reliable software.
  Software may need to be integrated with the libraries, databases and other program(s). This stage of SDLC is involved in the integration of software with outer world entities.
This means installing the software on user machines. At times, software needs post-installation configurations at user end. Software is tested for portability and adaptability and integration related issues are solved during implementation.
This phase confirms the software operation in terms of more efficiency and less errors. If required, the users are trained on, or aided with the documentation on how to operate the software and how to keep the software operational. The software is maintained timely by updating the code according to the changes taking place in user end environment or technology. This phase may face challenges from hidden bugs and real-world unidentified problems.
As time elapses, the software may decline on the performance front. It may go completely obsolete or may need intense upgradation. Hence a pressing need to eliminate a major portion of the system arises. This phase includes archiving data and required software components, closing down the system, planning disposition activity and terminating system at appropriate end-of-system time.
It’s always important to have a good and aesthetic design to please users
Be it any software it should be able to perform the functionality impeccably without issues
Durability is a confusing term, In this context, durability means the ability of the software to work without any issue for a long period of time.
Software should be able to perform consistently over platform and devices
Bugs associated with any software should be able to capture and fix quickly and news tasks and enhancement must be added without any trouble
customer and companies who make this app should feel that the money spent on this app has not gone to waste.
Have a clear idea about how the quality assurance process will be carried out through the project. Quality engineering activities required should also be set at the beginning along with team skill check.
Checkpoints at required intervals should be set
Software becomes vulnerable to attacks if it is inconsistent, contains bugs and errors in logic. Most of the aforementioned problems arise due to the faulty programming code that might have resulted from poor coding practices.
Poor coding has an adverse effect on the performance of the site. The performance issues comprise a multitude of things like when the user is interacting with the site, server response issues, reusability & flow of the code, etc.
It is originally designed for assessing modifiability, but later was extended for reviewing architecture with respect to quality attributes.
It is a polished and improved version of SAAM, which reviews architectural decisions with respect to the quality attributes requirements, and how well they satisfy particular quality goals.
 It is best suited for incomplete or in-progress architectures, which more focus on a set of issues or individual sections of the architecture at a time, rather than performing a general review.
It combines the ADR aspect of reviewing in-progress architecture with a focus on a set of issues, and the ATAM and SAAM approach of scenario-based review focused on quality attributes.
It focuses on analyzing the costs, benefits, and schedule implications of architectural decisions.
It estimates the modifiability of architecture for business information systems (BIS).
It estimates information system family architectures for interoperability and extensibility.
refers to a self-contained method for detecting, locating, and correcting faults with a software procedure. It exploits the structure of numerical operations.
the logic statements inserted at different points in the program reflect invariant relationships between the variables of the program and they often lead to various problems as assertions are not transparent to a programmer and their effectiveness depends on the nature of an application and on a programmer's ability.
 The basic task of CFC is to partition an application program in basic blocks or the branch-free parts of code. A deterministic signature (or number) is assigned to each block and faults are detected by comparing the run-time signature with a pre-computed one. In most CFC techniques one of the major problems is to tune the test granularity that should be used.
The programmer decides to duplicate the most critical procedures and to compare the obtained results on executing the procedures on two different processors. This approach requires a programmer to decide which procedures to be duplicated and to introduce proper checking on the results.
Computation results from master and shadow instructions are compared before writing to memory. Upon mismatch, the program jumps to an error handler that will cause the program to restart.
This approach relies on periodic reloading of code on main memory from an immutable memory. This is effective for protecting the code segment of Operating system and application programs. Performance penalty is due to repetitive memory reading.
This approach means running an application in the presence of faults. Few processors are used to run the same program and vote to identify errors in any single processor. Errors can be masked from application software. No software rollbacks are required to fix errors.
This means removing failed modules from the system. When failure occurs in a module, its effects on the remaining portion of the system is isolated. A large number of functional modules are used, which are switched automatically to replace a failing module.
This ensures reliability but is expensive in terms of hardware or runtime cost. The idea is to take a majority vote on a calculation replicated N times. Its software solution requires each processor to run N copies of surrounding computations and then vote on the result. This slows down the computation by at least a factor of N.
 Transient errors or soft errors are detected through time redundancy in the ReStore architecture. The novelty of the ReStore architecture is the use of transient error symptoms, such as, memory protection violation and incorrect control flow etc.
 This mechanism detects differences in execution across a dual modular redundant (DMR) processor pair. It summarizes a processor's execution history in a hash-based signature. Differences between two mirrored processors are exposed by comparing their fingerprints.
During the execution of a program if at any point of time a PSW is found to be an invalid one then we say that an error has occurred. Invalid PSW means it does not match to any one of our known or valid PSWs in the PSW bank meant for that program.
We apply various assertions that are derived from our understanding about the semantics of an application. Any violation at an assertion indicates an erroneous state.
They are effective for bit error detection but not suitable for error correction. The single parity checks can detect only odd number of single bit errors. Any even number of single bit-errors remains undetected.
 By using typical checksums for each row and column in a matrix, we can detect erroneous element of a matrix. This is useful for detecting errors in application data.
 This is useful for detecting transient errors or soft-errors in microprocessor registers, memory and stack area that might occur during the operational time at various industrial environments.
 These are agile tools that help us plan, track, and discuss our work, even with other teams.
These will let us build, test, and deploy with CI/CD that works with any language, platform, and cloud.
These are manual and exploratory testing tools.
 These provide unlimited, cloud-hosted private, and public Git repos.
 These let us create, host, and share packages.
 SDLC
 ISO
 DRE
 MTBF
 MTTR
 SAAM
 ATAM
 ADR
 ARID
 CBAM
 ALMA
 BIS
 FAAM
 ADL
 IEEE
 UML
 ABFT
 CFC
 EDDI
 EDAC
 DMR
 TMR
 BER
 FER
 PSWT
 ASBA